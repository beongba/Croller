#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.

from bs4 import BeautifulSoup
from selenium import webdriver
import time
import sys
import re
import math
import numpy  # pip install numpy 를 해야 사용가능함
import pandas as pd  # pip install pandas 해야 사용가능하고 numpy 가 먼저 설치되어 있어야

import random
import os   # 폴더 생성하기 위해 필요함
from selenium.webdriver.common.keys import Keys
import urllib.request
import urllib

#Step 2. 사용자에게 검색어 키워드를 입력 받습니다.

print("=" *80)
print(" 연습문제 : 조선일보 제목 뽑아오기 ")
print("=" *80)
print("\n")


date = input('언제부터 지금까지 랭킹 기사를 불러오실 건가요? (예:20200101)')


#query_txt = input('    크롤링할 키워드는 무엇입니까? : ')
#cnt = int(input('    크롤링 할 건수는 몇건입니까? : '))
print("\n")
#f_dir=input(' 파일이 저장될 경로만 쓰세요(기본경로 : c:\\temp\\ ) : ')
#if f_dir =='' :
#        f_dir = "c:\\temp\\"

print("    요청하신 데이터를 수집 중입니다")
print("\n")
print("    잠시만 기다려 주세요~~~~~^^")
print("\n")

'''
# 저장될 파일위치와 이름을 지정합니다
now = time.localtime()
s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)

os.chdir(f_dir)
img_dir = f_dir+s+'-'+query_txt
os.makedirs(img_dir)
os.chdir(f_dir+s+'-'+query_txt)

f_name=f_dir+s+'-'+query_txt+'\\'+s+'-'+query_txt+'.txt'
'''

# 날짜 지정하기
now = time.localtime()
s = '%04d.%02d.%02d' % (now.tm_year, now.tm_mon, now.tm_mday)

path = "c:/temp/chromedriver.exe"
driver = webdriver.Chrome(path)
driver.set_page_load_timeout(20)
driver.get('http://news.chosun.com/ranking/list.html?type=&site=&scode=&term=&date=' + date)
driver.maximize_window() # 화면 최대화

#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.
last_date='00000000'

while s != last_date :
    time.sleep(4)
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    last_date = soup.find('div',class_='month_calendar').find('strong').get_text().strip()
    print(last_date)
    #last_date1 =last_date.replace('.', '')
    content = soup.select('div.list_content div.list_inner span')
    
    no = 1
    for i in content :
        print(no, '위 : ', i.get_text())
        no += 1
    print('*' * 80)
    print('*' * 80)
    time.sleep(5)
    driver.find_element_by_xpath('//*[@id="btn_next_id"]').click()
driver.close()
