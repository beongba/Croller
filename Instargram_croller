#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.

from bs4 import BeautifulSoup
from selenium import webdriver
import time
import sys
import re
import math
import numpy  # pip install numpy 를 해야 사용가능함
import pandas as pd  # pip install pandas 해야 사용가능하고 numpy 가 먼저 설치되어 있어야

import random
import os   # 폴더 생성하기 위해 필요함
from selenium.webdriver.common.keys import Keys
import urllib.request
import urllib

#Step 2. 사용자에게 검색어 키워드를 입력 받습니다.

print("=" *80)
print(" 인스타그램 이미지, 해시태그 추출")
print("=" *80)
print("\n")

query_txt = input('    크롤링할 키워드는 무엇입니까? : ')
cnt = int(input('    크롤링 할 건수는 몇건입니까? : '))
print("\n")
f_dir=input(' 파일이 저장될 경로만 쓰세요(기본경로 : c:\\temp\\ ) : ')
if f_dir =='' :
        f_dir = "c:\\temp\\"

print("    요청하신 데이터를 수집 중입니다")
print("\n")
print("    잠시만 기다려 주세요~~~~~^^")
print("\n")

# 저장될 파일위치와 이름을 지정합니다
now = time.localtime()
s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)

os.chdir(f_dir)
img_dir = f_dir+s+'-'+query_txt
os.makedirs(img_dir)
os.chdir(f_dir+s+'-'+query_txt)

f_name=f_dir+s+'-'+query_txt+'\\'+s+'-'+query_txt+'.txt'

#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.

path = "c:/temp/chromedriver.exe"
driver = webdriver.Chrome(path)
driver.set_page_load_timeout(20)
    
driver.get('https://www.instagram.com/')
driver.maximize_window() # 화면 최대화

time.sleep(random.randrange(2,5))  # 2 - 5 초 사이에 랜덤으로 시간 선택
element = driver.find_element_by_xpath('//*[@id="loginForm"]/div/div[1]/div/label/input')
element.send_keys('') # 아이디 입력(input으로 넣을수도 있음)

    
time.sleep(3)
element = driver.find_element_by_xpath('//*[@id="loginForm"]/div/div[2]/div/label/input')
element.send_keys('') # 비밀번호 입력(input으로 넣을수도 있음)
time.sleep(3)
element.send_keys('\n')
time.sleep(3)
try :
    driver.find_element_by_xpath('//*[@id="react-root"]/section/main/div/div/div/div/button').click() # 나중에 하기 클릭
except :
    print('아이디와 비밀번호를 확인해주세요')
time.sleep(3)
driver.find_element_by_xpath('/html/body/div[4]/div/div/div/div[3]/button[2]').click() # 나중에 하기 
                            

driver.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]').click() # 검색클릭

element = driver.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]/input')
#단어 검색
element.send_keys(query_txt)
time.sleep(2)
element.send_keys('\n')
element.send_keys('\n')

time.sleep(3)

driver.find_element_by_xpath('//*[@id="react-root"]/section/main/article/div[1]/div/div/div[1]/div[2]/a').click()
# 상세 페이지 클릭

no = 1
file_no = 1
num = []
picture = []
h_tag = []

while True :
    time.sleep(2)
    
    num.append(no)
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    
    try :
        content = soup.find('div', class_='_97aPb').find('img')['src'] # 이미지 뽑기
    except :
        print('동영상은 Pass 합니다.')
        pass
    else :
        print("%s 번째 이미지 저장중입니다=======" %file_no)
        print(content)
        picture.append(content) 
        urllib.request.urlretrieve(content,str(file_no)+'.jpg')
        print("\n")
        print("%s 번째 해쉬태그 저장중입니다=======" %file_no)
        content = soup.select_one('div.eo2As div.C4VMK')
        content1 = content.find_all('a', class_='xil3i') # 해쉬태그
        f = open(f_name, 'a', encoding = 'UTF-8')
        for i in content1 :
            time.sleep(0.1)
            print(i.get_text())
            h_tag.append(i)
            f.write(i.get_text() + "\n")
        f.close()
        print("="* 80)
        
        if no == cnt :
            break
        no += 1
        file_no += 1 
    
    driver.find_element_by_link_text('다음').click()

print('작업을 마치고 종료합니다.')
driver.close( )
#c:\jupyter_work\
